import io
import time
import torch
from pydub import AudioSegment
from speech_recognition import Recognizer, Microphone
from transformers import Wav2Vec2Processor, Wav2Vec2ForCTC


# load model and tokenizer
processor = Wav2Vec2Processor.from_pretrained("nguyenvulebinh/wav2vec2-base-vietnamese-250h")
model = Wav2Vec2ForCTC.from_pretrained("nguyenvulebinh/wav2vec2-base-vietnamese-250h")

recognizer = Recognizer()

start_time = time.time()
with Microphone(sample_rate=16000) as source:
  print("You can start speaking now...")
  while True:
    audio = recognizer.listen(source, phrase_time_limit=1)
    data = io.BytesIO(audio.get_wav_data())
    clip = AudioSegment.from_file(data)
    x = torch.FloatTensor(clip.get_array_of_samples())

    # tokenize
    input_values = processor(x, sampling_rate=16000, return_tensors="pt", padding="longest").input_values  # Batch size 1

    # retrieve logits
    logits = model(input_values).logits

    # take argmax and decode
    predicted_ids = torch.argmax(logits, dim=-1)
    transcription = processor.batch_decode(predicted_ids)

    print('You said: ', str(transcription))